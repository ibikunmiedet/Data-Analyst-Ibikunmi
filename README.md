# Data-Analyst-Ibikunmi
# PROJECTS DONE FOR CITY OF VANCOUVER (COV) & UNIVERSITY CANADA WEST (UCW)
# CITY OF VANCOUVER DRAW.IO DESIGN FILE![City of Vancouver AWS DAP 2](https://github.com/user-attachments/assets/cb4c6213-59f4-4564-a5a4-c4a63bd98328)

# Project 1: Exploratory Data Analysis (COV)

Project Description: Exploratory Data Analysis (EDA) on City of Vancouver Rental Standards- Current Issues Dataset
  
Project Title: The Vancouver Rental Standards and Current Issues Data Analysis using AWS Cloud Services
  
Objective: The main goal of the project is to effectively analyze the City of Vancouver’s -rental standards dataset, in order to identify issues like the total outstanding housing unit payments my business operators in the metro-Vancouver area, identify each business key location addresses, unique business URL, total building units occupied by each business by coordinate points. This will provide information to improve housing standards, regulate rental prices and improve policy making in the city.

Dataset: The City of Vancouver Dataset provides insight using some specific variables, including details such as:  
• Name of Business Operator
•	Detailed Business URL
•	Street Number of Business 
•	Street Name 
•	Total Units occupied by the business
•	Total Outstanding 
•	Geometric Location (Coordinates) 
•	Geo Local Area location
•	Geo Point 2D Coordinates

Methodology:
- Data Collection and Preparation:![Screenshot 2025-02-28 184239](https://github.com/user-attachments/assets/1a76275c-7c50-4a8e-96e0-8ad0e51f6d5e)

- Download the dataset from the City of Vancouver data portal in CSV format and load the dataset using Microsoft Excel.
- Perform initial data cleaning, which includes handling missing values, correcting data types, and renaming columns for clarity.
  
1-	Data Profiling:![Screenshot 2025-03-01 225112](https://github.com/user-attachments/assets/fe219eda-1794-477d-b42a-365ab2daa9bb)

- Patterns, Outliers, and the distribution of the data will be identified.
  
2-	Data Cataloging 
- Conversion of data and transformation of data structures.
  
3-      Summarization:
- Understanding key data characteristics, and performing analysis for insights
  
4-	Data Analysis:
- Looking at how the analysis done changes the data, and what we can infer from the results
  
Tools and Technologies:
•	AWS S3 for Data Storage and ingestion 
•	AWS EC2 Instances
•	Remote Desktop Computer, RDP 
•	AWS Glue Data Brew for Data Profiling and Cleaning 
•	AWS Glue and AWS Crawler for Data Cataloging
•	AWS Glue for Data Summarization
Deliverables:
•	Report summary on common violators and rental issues 
•	Interactive insights showcasing rental standards and trending issues

# Project 2: Descriptive Analysis (COV)
Project Description: Descriptive Analysis of City of Vancouver Rental Standards- Current Issues Dataset

Project Title: The Vancouver Rental Standards and Current Issues Descriptive Analysis using AWS Cloud Services

Objective: The main goal of the project is to effectively analyze the City of Vancouver’s rental standards dataset for the provision of business operators' rental conditions and understand statistical insights into total units and outstanding violations of business locations.

Dataset: The City of Vancouver Dataset provides insight using some specific variables, including details such as:

• Name of Business Operator 

•	Detailed Business URL

•	Street Number of Business

•	Street Name 

•	Total Units occupied by the business

•	Total Outstanding 

•	Geometric Location (Coordinates) 

•	Geo Local Area location

•	Geo Point 2D Coordinates


Methodology:

1-	Data Cataloging ![Screenshot 2025-03-04 182553](https://github.com/user-attachments/assets/78a16d7e-c4c7-4b9f-a910-deb631bd1918)


2-	Data Visualization ![Screenshot 2025-03-04 195921](https://github.com/user-attachments/assets/9d6eaf41-c623-4e28-aa1a-73ecdb5a8723)


3-	Data Summarization ![Screenshot 2025-03-04 200403](https://github.com/user-attachments/assets/c20b356c-046d-413d-ad7a-8590917e4712)


4-	Business Question SQL Queries 


Tools and Technologies:

•	AWS S3 for storage 

•	AWS Glue 

•	AWS Glue DataBrew

•	Amazon Athena 
![Screenshot 2025-03-05 173645](https://github.com/user-attachments/assets/b43e4713-45c6-4da8-a9b7-63af253c22ae)
![Screenshot 2025-03-05 173631](https://github.com/user-attachments/assets/69d7e38f-2c61-48de-9ef9-64150b484836)
![Screenshot 2025-03-05 173613](https://github.com/user-attachments/assets/d6bdc755-2390-4ee3-9843-c69cebf1b3e6)



Deliverables:

•	Summary of the rental conditions of businesses across various geo-local areas

•	Reports on the trends in issues businesses face in the City of Vancouver 


# Project 3: Data Wrangling (COV)![Screenshot 2025-03-02 214841](https://github.com/user-attachments/assets/d033b738-7b0c-4c78-a8d1-a949b466afe4)

Project Description: Data Wrangling for City of Vancouver Rental Standards- Current Issues Dataset

Project Title: The Vancouver Rental Standards and Current Issues Data Wrangling using AWS Cloud Services 

Objective: The primary goal of this project is to clean, transform and adequately prepare the rental standards and current issues dataset for further analysis.

Dataset: The City of Vancouver Dataset provides insight using some specific variables, including details such as:

•	Name of Business Operator 

•	Detailed Business URL

•	Street Number of Business 

•	Street Name 

•	Total Units occupied by the business

•	Total Outstanding 

•	Geometric Location (Coordinates) 

•	Geo Local Area location

•	Geo Point 2D Coordinates


Methodology:

1-	Data Ingestion: Extracting the data and loading the data from the AWS S3 bucket.

2-	Data Cleaning: All inconsistent formatting, data faults, and issues like missing values or null values.

3-	Lifecycle configuration- based on the frequency of access to the datasets. Standard access was chosen.

4-	Data Cleaning Cost Analysis: Insight all how much the cleaning process cost the city.


Tools and Technologies:

•	AWS Glue DataBrew ![Screenshot 2025-03-04 201534](https://github.com/user-attachments/assets/c1048257-3ce2-4fdd-ba28-22d62a944684)


•	Amazon EC2 Instances 


Deliverables:
•	A cleaned and transformed rental standards and current issues dataset ready for analysis, available in a suitable format (e.g., CSV, Excel Database).

•	A comprehensive report documenting the data wrangling process, including challenges encountered, methods employed, and final dataset characteristics.


# Project 4: Data Quality Control (COV)![Cloudtrail](https://github.com/user-attachments/assets/14e4e8a2-4c98-4248-9175-3a70301a56cf)
![All alarms](https://github.com/user-attachments/assets/b13dcc24-a4d0-4d0d-ab13-5714f1c06420)
![Alarms in MCR dashboard](https://github.com/user-attachments/assets/21c3ab96-29a4-4a56-b61a-a11af050828e)

Project Description: Data Quality Control Initiative for City of Vancouver Rental Standards- Current Issues Dataset

Project Title: Implementation of Data Quality Control Measures at the City of Vancouver Data Center Portal to ensure confidentiality, adequate protection, and integrity of the data.

Objective: The primary objective of this project is to establish a comprehensive Data Quality Control (DQC) framework for the City of Vancouver, ensuring that it is accurate, reliable and valid data used in all analysis.

Dataset: The City of Vancouver Dataset provides insight using some specific variables, including details such as:

•	Name of Business Operator 

•	Detailed Business URL

•	Street Number of Business 

•	Street Name 

•	Total Units occupied by the business

•	Total Outstanding 

•	Geometric Location (Coordinates) 

•	Geo Local Area location

•	Geo Point 2D Coordinates


Scope: The project will focus on the following key areas:

•	Data Security: Analyzing existing datasets to assess the levels of security being applied, data encryption, decryption, and key management access![Raw Default Encryption](https://github.com/user-attachments/assets/9cba2a97-bc77-4914-a98c-9ca3d9320fa8)
![Raw Bucket version](https://github.com/user-attachments/assets/64faf3bd-079b-4334-b3b0-31b52a0d5590)
.

•	Data Governance: Developing new pipelines, using visual ETL, checking for failed and passed quality checks and data quality rule applications.

•	Data Monitoring: Establishing ongoing monitoring processes and dashboards to track data quality metrics using alarms, and thresholds.

•	Training and Awareness: Creating training programs for staff on data quality best practices.


Methodology:

1-	Data Analysis: Business Question queries using SQL and Amazon Athena.

2-	Creating a Symmetric Key with a key management system and enabling default encryption and bucket versioning.![Housing Key Management](https://github.com/user-attachments/assets/2f83a394-d0c9-4c67-9d58-d259aeabc4d2)


3-	Bucket Replication: New bucket folders for the quality check results in AWS S3.
![trf replication rule](https://github.com/user-attachments/assets/5bb8d77d-15f9-4787-b0fb-91709437992b)
![raw replication rule](https://github.com/user-attachments/assets/080e3c9e-eea2-4cb1-aa94-2d92c0ece45c)
![cur replication rule](https://github.com/user-attachments/assets/5a7ae11a-6c0b-4dce-9be7-4f4aa463efda)


4-	Governance implementation using quality check, quality rules and Visual ETL techniques in AWS Glue 
![Visual ETL](https://github.com/user-attachments/assets/caa48de1-8b64-44a8-af25-dc03c7571a60)
![Passed Data quality](https://github.com/user-attachments/assets/9030dd80-cf64-4d31-b507-1d51d24cc1d6)
![Failed data quality](https://github.com/user-attachments/assets/0485284c-4926-48b0-b43c-233007751b81)


5-	AWS Cloudwatch monitoring dashboard, custom line graphs, alarm, and threshold implementation, Amazon SNS, and lastly, cloud trail log for all user activities.
![alarm threshold cur](https://github.com/user-attachments/assets/6fb0a4e1-765c-48cc-adfa-17688d323798)
![Alarm and Threshold TRF](https://github.com/user-attachments/assets/905dea8e-2608-48fd-a79e-242b3db8bd37)
![Alarm and Threshold Raw](https://github.com/user-attachments/assets/46b690c2-5fb9-4c14-859a-cc878dae6ecc)


6-Training and Best Practices: Train all City Data Center staff and operations teams and implement best practices in DAP operations based on AWS Pillar recommendations like operational excellence, security, reliability, performance efficiency, and lastly, cost optimization.


Tools and Technologies:

•	AWS S3

•	AWS Glue

•	AWS Cloudwatch 

•	AWS Cloudtrail![Cloudtrail event history](https://github.com/user-attachments/assets/0bfa0a77-54b3-4660-a6fb-e7162af195bb)


•	AWS Key Management System


Timeline:
The projects listed above will empower the City of Vancouver data center to enhance its data integrity and reliability, resulting in improved decision-making, operational efficiency, and compliance with regulatory requirements. It lasted for a period of 11 weeks from start to completion.

# University Canada West DRAW.IO Design File ![Screenshot 2025-03-26 230519](https://github.com/user-attachments/assets/fdcb0f59-ac95-4be0-a4d6-3ce3a3a01eff)

# Project 5: Exploratory Data Analysis (UCW)
Project Description: Exploratory Data Analysis (EDA) Initiative for University Canada West Registrar’s Office Students rights and responsibilities Dataset

Project Title: The Analysis of the University Canada West’s Registrar’s office Student Rights and Responsibilities Issues Data Analysis using AWS Cloud Services 

Objective: The main goal of the project is to effectively analyze the University’s data portal, looking especially into the student's rights and responsibilities database. Inferring on past data and user activities on misconduct panels, student complaints, community member information, and results of panels. This will provide information to improve student rights, provide better insight into the responsibilities of students, and improve policy-making in the university.

Dataset: The three University Datasets provide insight using several specific variables, they include: 

•	Community Member List (breakdown of community member names, Unique ID’s, Programs enrolled, and year of study)

•	Misconduct List- (misconduct complaints, reviews, panels, results, community member ID) 

•	Complaint List- (Complaints made, status of complaints, community member ID, date of complaints)


Methodology:

1-	Data Collection and Preparation: Download the dataset from the University data portal in CSV format and load the dataset using Microsoft Excel. Perform initial data cleaning, which includes handling missing values, correcting data types, and renaming columns for clarity.

2-	Data Profiling: Patterns, Outliers, and the distribution of the data will be identified.

3-	Data Cataloging: Conversion of data and transformation of data structures.

4-	 Summarization: Understanding key data characteristics, and performing analysis for insights

5-	Data Analysis: Looking at how the analysis done changes the data, and what we can infer from the results


Tools and Technologies:

•	AWS S3 for Data Storage and ingestion

•	AWS EC2 Instances

•	Remote Desktop Computer, RDP 

•	AWS Glue Data Brew for Data Profiling and Cleaning 

•	AWS Glue and AWS Crawler for Data Cataloging

•	AWS Glue for Data Summarization


# Project 6: Descriptive Analysis (UCW)
Project Description: Descriptive Analysis University Canada West Registrar’s Office Students rights and responsibilities Dataset

Project Title: The UCW Registrar’s Office Student Rights and Responsibilities Descriptive Analysis using AWS Cloud Services 

Objective: The primary goal is to objectively analyze all three datasets and infer descriptive analytical insights from them for better policymaking.

Dataset: The three University Datasets provide insight using several specific variables, they include:

•	Community Member List (breakdown of community member names, Unique ID’s, Programs enrolled, and year of study)

•	Misconduct List- (misconduct complaints, reviews, panels, results, community member ID) 

•	Complaint List- (Complaints made, status of complaints, community member ID, date of complaints)


Methodology:

1-	Data Cataloging ![UCW Data Catalog tables](https://github.com/user-attachments/assets/5d6ceb02-467d-4f50-b59c-8ba10355f560)


2-	Data Visualization 
![UCW Community summariztion](https://github.com/user-attachments/assets/84fcbc74-0c2a-4e96-8eaa-b0e63d21287f)
![UCW Misconduct summarization](https://github.com/user-attachments/assets/839a23e2-bf96-4207-a7c0-a856c78d85ae)



3-	Data Summarization

4-	Business Question SQL Queries

Tools and Technologies:

•	AWS S3 for storage 

•	AWS Glue 

•	AWS Glue DataBrew

•	Amazon Athena


# Project 7: Data Wrangling (UCW)
Project Description: Data Wrangling for University Canada West Registrar’s Office Students rights and responsibilities Dataset

Project Title: The University Canada West Registrar’s Office Students rights and responsibilities Data Wrangling using AWS Cloud Services 

Objective: The primary goal of this project is to clean, transform and adequately prepare the registrar’s office dataset for further analysis.

Dataset: 

•	Community Member List (breakdown of community member names, Unique ID’s, Programs enrolled, and year of study)

•	Misconduct List- (misconduct complaints, reviews, panels, results, community member ID) 

•	Complaint List- (Complaints made, status of complaints, community member ID, date of complaints)


Methodology:

1-	Data Ingestion: Extracting the data and loading the data from the AWS S3 bucket.

2-	Data Cleaning: All inconsistent formatting, data faults, and issues like missing values or null values.

3-	Lifecycle configuration- based on the frequency of access to the datasets. Standard access was chosen.

4-	Data Cleaning Cost Analysis: Insight all how much the cleaning process cost the city.

Tools and Technologies:

•	AWS Glue DataBrew 

•	Amazon EC2 Instances


# Project 8: Data Quality Control (UCW)
Project Description: Data Quality Control Initiative for University Canada West Registrar’s Office Students rights and responsibilities Dataset

Project Title: Implementation of Data Quality Control Measures at the University Canada West Department of Students Rights and responsibilities to ensure confidentiality, adequate protection, and integrity of the data.

Objective: The primary objective of this project is to establish a comprehensive Data Quality Control (DQC) framework for the University of Canada West, ensuring that it is accurate, reliable, and valid data used in all analyses.

Scope: The project will focus on the following key areas:

•	Data Security: Analyzing existing datasets to assess the levels of security being applied, data encryption, decryption, and key management access.![UCW KMS](https://github.com/user-attachments/assets/ae1da887-fff3-43bd-8114-2717e6f4f084)


•	Data Governance: Developing new pipelines, using visual ETL, checking for failed and passed quality checks and data quality rule applications.![UCW Visual ETL QC](https://github.com/user-attachments/assets/c1011796-9663-4294-8b4f-fb6cbff95a92)
![UCW Passed quality check](https://github.com/user-attachments/assets/55260062-9223-4051-ad2a-708641af00b3)
![UCW Failed QC](https://github.com/user-attachments/assets/7ea70f84-b43d-4e5d-9c06-b3423c2b6de5)


•	Data Monitoring: Establishing ongoing monitoring processes and dashboards to track data quality metrics using alarms, and thresholds.

•	Training and Awareness: Creating training programs for staff on data quality best practices.

Methodology:

1-	Data Analysis: Business Question queries using SQL and Amazon Athena.

2-	Creating a Symmetric Key with a key management system and enabling default encryption and bucket versioning.![UCW KMS](https://github.com/user-attachments/assets/16a41abb-17fa-45a9-b3a2-9271646563ff)


3-	Bucket Replication: New bucket folders for the quality check results in AWS S3.

4-	Governance implementation using quality check, quality rules and Visual ETL techniques in AWS Glue

5-	AWS Cloudwatch monitoring dashboard, custom line graphs, alarm, and threshold implementation, Amazon SNS, and lastly, cloud trail log for all user activities.

6-      Training and Best Practices: Train all City Data Center staff and operations teams and implement best practices in DAP operations based on AWS Pillar recommendations like operational excellence, security, reliability, performance efficiency, and lastly, cost optimization.

Tools and Technologies:

•	AWS S3

•	AWS Glue

•	AWS Cloudwatch ![UCW Alarm](https://github.com/user-attachments/assets/b501b65e-2b15-4807-b1a6-f59652477746)


•	AWS Cloudtrail ![UCW Cloudtrail](https://github.com/user-attachments/assets/13242111-3f72-4913-869a-b1672f056ba9)


•	AWS Key Management System



# Brief Report on City of Vancouver Projects (Similar implementations were done with University Canada West)
# DAP Design and Implementation
The draw.io file has been used to design the move of the physical on-premises data center for the City of Vancouver because of issues and headaches. The AWS data center is indicated, as well as the region of Virginia, and the availability zone is 1-a, which is the US East availability zone. The operational environment for the housing rental standards department office of the City of Vancouver is also indicated. The respective operation teams are responsible for transferring data from the source which is the operational environment, managed by the operations team. The data from the operational environment is stored and processed by the City of Vancouver employees. We ingest data from the Operational environment to the Data Lake in the Data Analytics Platform.
The data team manages the data analytic platform (DAP). They are in charge of all processes that involve storing data, communicating with, and processing data in the DAP using AWS services and tools. 
Data is being ingested from the general server for storing data, the specific virtual server (Platform) web server is important because it grants users and citizens access to the website of the city of Vancouver data portal. The web server is used to track user interactions in the form of clickstream. EC2 instances help to run the web server, and the data from the web server can be stored in the S3 buckets too. The datasets are saved in the S3 buckets based on their data ingestion rates. The web Application server runs on Elastic Beanstalk. There are three S3 buckets: raw, transform, and curated buckets. The job icon and AWS Glue Databrew are signs that tell us that jobs were run for data profiling and cleaning. 
AWS crawler is used to run the data catalog and convert semi-structured or unstructured data to structured data in the form of tables instead of rows. Additionally, AWS Glue visual ETL pipeline is also used to summarize the data and break it down into more understandable forms based on the needs of the user.
The current-issues dataset with primary key- Business Operator and Foreign Key- GeoLocal Area, is stored in the S3 buckets in csv format, while the housing-user-log is stored in text format.
The data cleaning cost analysis table shows us the cost of the cleaning process of the dataset using the recipe job details and overview for the cleaning analysis. Similarly, the profile jobs provide details for job profiling. Data summarization cost analysis breaks down the cost of data techniques used in the ETL pipeline. Likewise, the data ingestion cost analysis serves as a breakdown of the communication, processing, and storing costs.
AWS Amazon Athena and SQL were implemented in the design to run queries and analysis for descriptive analysis.

# Descriptive Analysis
The dataset is a relational data model and exploratory data analysis (EDA) was performed to summarize data to extract patterns. SQL queries were run to produce new tables using the data catalog and the AWS Athena. The queries were stored in the curated S3 bucket housing-cur-edet. The first command entered in the editor was select * from "hou_curr_iss_trf_system"; to give all the rows from this dataset. To reduce the cost, instead of receiving all the rows each time the query is run, we limited the rows a sample of 20 rows using the select * from "hou_curr_iss_trf_system" limit 20; command.
The descriptive analysis was based on distribution and time-series trends to find the average, max, and sum respectively. The command queries for the dataset are as follows: 

•	Business_Question_1: What is the average number of total units amongst business operators in the City of Vancouver?
select avg(totalunits) as totalunits_avg from "hou_curr_iss_trf_system" limit 20; This command produced one row and one column, giving the average of total units of business operators in the City of Vancouver dataset. 

•	Business_Question_2: What are the minimum average total units amongst business operators in the City of Vancouver?
select min(totalunits) as totalunits_min, avg(totalunits) as totalunits_avg from "hou_curr_iss_trf_system" limit 20; This command produced one row and two columns, giving the minimum total units average for business operators.

•	Business_Question_3: What are the average total units for the different Geolocal areas? Group by GeoLocalarea and for each Geolocalarea, calculate the average number of total units. This is the command, it gave us 20 rows and 2 columns. select geolocalarea, avg(totalunits) from "hou_curr_iss_trf_system" group by geolocalarea limit 20;.

# Step 1: Data Ingestion 
We have our dataset rental_standards_current_issues in CSV downloaded from the City of Vancouver official data portal.
/rentalstandards/currentissues/year=25/quarter=01/month=01/server=HGVS-Edet
The data is going to be ingested/updated on a monthly basis for the Housing Office Data teams use. 
The Housing Office department ingesting data from the city of Vancouver- Housing office department operational environment and are storing the data in the Data Analytics Platform (DAP) through data ingestion.  The result of my data ingestion is going into the data lake. 
To implement the data lake, we are using S3 services. Making use of buckets (Housing-raw-edet) that will store all files/objects from the city of Vancouver Housing General Virtual Servers (HGVS-Edet).
The virtual server acts exactly as a physical server. We use it in the implementation process. We use the EC2 instance service to implement the virtual server. The HGVS-Edet instance uses the Windows Operating system as the Amazon Machine Image (AMI). The instance type- virtual CPU, hard disk or NIC used for the server, helps us calculate the price paid using the virtual computer running per hour based on the chosen configuration- t3.micro. Key pair is used to secure the credentials. 
Vockey technique was selected. Network settings default configurations were used, getting access to the computer from “anywhere” using the “Allow RDP traffic from”. The virtual server is given permission to perform data ingestion using the security IAM role modification by using LabInstanceProfile as the IAM role. Windows PowerShell was used to write commands to ingest the dataset and store it in the S3 bucket. 
Connecting the computer, we use RDP client technique. Using the machine Public DNS, Username and Password. The remote desktop connection software was used to connect virtual computers through RDP client. In the windows remote desktop, a new text document for the rental standards dataset was created as “currentissues.txt”. We copied the content of the current issues dataset in CSV format and copied it into the remote desktop txt. document and changed the format to CSV. We copied the URL address of the Vancouver Housing General Virtual Servers (HGVS-Edet), copied the “currentissues.csv” files as a path and entered the required code in the Windows Powershell.

# Step 2: Data Profiling
The specific server is added because of specific software being added. The web server is an important source of user activity information. We observed the dataset to understand the dataset issues. The dataset had few invalid values, missing values, and structure issues after reviewing 17 possible data issues.   
Additionally, the data ingestion cost analysis was also carried out to understand the cost of the data ingestion using the different parameters in the data ingestion cost analysis table. The cost of the data team started from the data ingestion and the data storage, including the communication cost of the object stored in the S3 bucket. The standard storage class is used, and the number of read requests is monthly basis.
AWS Glue DataBrew was used for profiling, a new project was created. The project is “Hou-curr-iss-prj-Edet”. The recipe name “Hou-curr-iss-prj-Edet-recipe”. A new dataset name was created “hou-curr-iss-ds-edet. The dataset is stored in the raw bucket in S3, in the current issues folder. It is stored in CSV format, and the LabRole was given permission. 
The project was created, and we used data profile overview to understand data issues. A job was created for the dataset and was stored in the S3 housing-trf-edet bucket. LabRole is also used as permission. The job was run, the data profile overview shows all the information about the job.

# Step 3: Data Cleaning
AWS Glue DataBrew was used to clean the dataset in the project after the profiling job was run. The dataset was observed for cleaning for any invalid or missing values. Additionally, each column name was renamed to one word, removing spaces or special characters, and the change was reflected in the defined recipe steps.
A job was created to run the project dataset. It is called “hou-curr-iss-cln-edet”. The result of the cleaning was stored in multiple formats. One of them being csv format, saved in the S3 bucket, and no compression for all users.
 In output settings 1, we replace output files for each job run. The location of the cleaned result is in the transform bucket in the User folder.
In output settings 2, we use PARQUET for large datasets, and all the information will be stored in form of column. It was compressed to reduce the storage costs using snappy. We also replace output files, and partition columns into multiple subfolders based on the categorical values in the dataset. The GeoLocal Area column was selected for the column partitioning. It was stored in the System folder. The clean User and System datasets were stored in the S3 housing-trf-edet bucket

# Lifecycle Configuration 
The Lifecycle rules were not changed since the dataset in the S3 buckets is accessed on a monthly basis. Therefore, the standard class is appropriate for the specific dataset since it is frequently accessed.

# The housing-user-log file![Screenshot 2025-03-02 232225](https://github.com/user-attachments/assets/fdaf8968-4923-440d-a186-7623c58ce7c2)

 /rentalstandards/housing-user-log/year=25/quarter=01/month=01/day=20/server=HSVS-Edet
The log file will be ingested, it has the click stream data, and the ingestion rate of the dataset is daily. It was stored in the S3 housing-raw-edet bucket. No lifecycle rule was created for it because the dataset is accessible daily.
A new HSVS-Edet EC2 instance was launched for this ingestion. The same procedures as the last instance launch were followed. The difference in this case is that this computer is available for two groups in the network settings. The operation team connects using the remote desktop, while the end users connect using a browser. We created a website for the end user to connect using simple http, https protocol.
A security group was created to give who we want access and how they can connect. A security group name was given. The rdp rule was used to give connection access to housing operation team and, http/https for end users to connect from anywhere in the world in secure and insecure ways. The EC2 instance was given permission using LabinstanceProfile.
The computer used is a specific web server, using EC2 to add platform to the computer. To change the general computer to a specific computer, we installed web server software using web server manager.
The html page in the wwwroot folder in the RDP Windows desktop is the website shown to clients as a default page. The insecure default webpage is connected using the HSVS-Edet Public IP address: 3.81.158.36. 
We went to the City of Vancouver Data Portal rental standards-current issues webpage and copied the html code of the webpage to direct the end user to that webpage. We changed the iisstart.html to iisstart.txt to make it a txt. File. Replaced the existing code with the html codes of the City of Vancouver Rental Standards and Current Issues web page codes, renamed the file to currentissues.html- The new website. 3.81.158.36/currentissues.html
The Log folder in the log files in inetpub shows us the clickstream information. The powershell is used to ingest the housing-user-log to the S3 HSVS-Edet storage folder.

# Step 4: Data Cataloging
Once the dataset has been cleaned, we need to have a structured format in tabular form and summarize the dataset. A new service called AWS Glue was used to convert semi-structured or unstructured data to structured data in a tabular form.
Any information stored in row csv or parquet format, will be converted to a collection of tables. To have the tables, a data catalog will be created, serving as a database or a collection of tables for the dataset. It is used for storing schema corresponding to the data stored in the housing-trf-edet S3 bucket. The conversion of the data structure was done through the discovery service called AWS crawler, housing-crw-edet. The system-friendly file types from the dataset are used as sources for the crawler fetching procedure.
A database was created for the transformation process. The new database was called housing-data-catalog-edet. It is populated with tables that have been created using the crawler concept. The preferred data source is from the housing-trf-edet system dataset, which are the result of the data cleaning process. Labrole was assigned for permission purposes in the configuration of the security settings. The corresponding prefix of the table is hou_curr_iss_trf_ and the schedule is on demand. 
The crawler was run in the Glue service, and the table created in the database. The schema shows all the column names and datatypes, as well as the location of the exact schema.

# Step 5: Data Summarization 
The ETL pipeline concept in the AWS Glue Service is used for this process. ETL means Extraction phase, Transformation phase, and Load phase respectively. The extraction part means we take detailed dataset from the data catalog and we transform, and save the result called load in a new S3 bucket called the Curated bucket- housing-cur-edet. The related metrics datasets will be stored in this bucket.
Additionally, a final specific server in the operational environment computer called the Housing Web Application (HWA-Edet) was implemented using the Beanstalk service instead of the EC2 service. A website was created to show some functionality inside this website, AWS creates EC2 instances, installs the web server, receives web application codes, and it is fully managed by AWS. The housing-app-log will be used to analyze application activity. It is a txt. Log file. The housing-app-log folder is stored in the housing-raw-edet bucket. 
A new bucket called the housing-cur-edet bucket was also created in the S3 storage service for the results of the summarized dataset. The metrics folder was created in the rental standards folder in this new curated bucket. Each summarized result is a metric, and it is divided into system and user.
The visual ETL pipeline name is called Current-Issues-Summarization and permission was granted to it using Labrole. We extracted the information from the data catalog and then went to the source tab for extraction, while the transform tab and techniques were used for summarization, while the target tab was used for storage. 
AWS Glue Datacatalog was used to configure and extract the data from the catalog, it is called Extract-curr-iss. The change schema option was used to drop the geometric coordinates column since the geopoint2D column is also given sharing similar information. The number of Columns went from 9 to 8. 
The filter operation was used to remove rows by adding a condition that used the key- total outstanding, operation- >, value- 5. That is filter if the total outstanding for each business operator is greater than 1. Additionally, I also added another condition using the “Global AND” option. The second condition showed the business operators in the downtown area. Key- GeoLocalarea, operation- matches, value- Downtown. This technique reduced the number of rows to 46 rows. The table now shows the business operators in the Downtown area with total outstandings greater than 1.
The aggregate function under the transformation operation was used to group related rows together, they are different but have similar properties, and aggregate operations were used to give summarized outputs. The best field for grouping is a categorical column, a string column. All records for the same field to group by- street were summarized by calculating the average total unit and average total outstanding for each street. Giving two metrics average total units and average total outstanding. There are now 20 rows and 3 columns 
Dates were added to know when the report was created for managerial information using Add current timestamp. It was named Add date to Summary Report. The Report_Date column was created, the format was changed using the timestamp format option yyyy-MM-dd HH:mm. 
Derived Column technique was used to convert the time format to local time zone, it is called the Conver to LTZ, and the new column is called Report_Date_LTZ. The SQL expression used is from_utc_timestamp(Report_Date, 'America/Vancouver') and a new column is formed. We went back to the change schema step to do some cleaning to prepare for load, to drop the previous report date and rename the new local time zone, remove brackets and rename columns. 
Storing the summarized data means using the target phase of the ETL to load in the metrics folder, system subfolder of the new curated bucket, housing-cur-edet in the S3 service, and the data catalog will also be updated. The new updated table is called hou-currentissues-metrics, and Report_Date and Street are added as partitions 0 and 1.
Autobalancing process is used to help users see information in just one file, with 1 partition. It is called convert to one file. Moving on to the next step to load for users, we changed the format to csv, compression is none, and we do not update the table. The ETL visualization job was run, and we can see the result.

# Step 6: Data Security
The Key Management Service was used to create a key. The symmetric key means that the key can be used for both operations, decryption and encryption.  The symmetric key was called hou-curr-iss-key-edet related to the housing department rental standards, mainly focused on the current issues. 
Granting access to the key is based on the group of users known as key administrators based on the sensitivity. Administrators can create, delete, and rotate keys. LabRole was used to attach new permissions to the role for administrators and users. 
Bucket replication for each bucket involved the creation of new backup buckets named housing-raw-bac-edet, housing-trf-bac-edet, and housing-cur-bac-edet respectively. AWS automatically replicates the content of the original raw, trf, and cur buckets to the backup buckets using the replication rule.

# Step 7: Data Governance  
	A new pipeline was added to the DAP for data quality checks using the AWS Glue service. A new folder called the Quality check folder was created in the transform dataset housing-trf-edet current issues dataset, and two subfolders named Passed and Failed were created in the Quality Check folder. The passed and failed results will be stored in the respective folders.
	A new visual ETL was created in AWS Glue called the hou-curr-iss-QC-Edet, and LabRole was the permission assigned to the job. The first thing done was the current issues dataset from the housing-raw-edet bucket was extracted, and the first step applied in the ETL was to Load Current Issues. The second node was to evaluate the data quality using three different rule types: 
 
•	Completeness (Passed): completeness “business_operator” >= 0.95. Stating that the completeness of each business operators name should be more than or equal to 95%.

•	Uniqueness (Failed): Uniqueness “street” > 0.50. Stating that the street names for each business should be unique to each business location by at least 50%.

•	Unique Value Ratio (Passed): UniqueValueRatio “streetnumber” > 0.95. Stating that the ratio of the street number to each business should be unique to a ratio of 0.95. 

# Step 8: Data Monitoring
AWS Cloudwatch was used to create a monitoring dashboard called hou-curr-iss-MCR-edet, it helped us to gather the result of metric measurements. The S3 services storage metrics were selected and analyzed. The housing-raw-edet, housing-trf-edet, and housing-cur-edet zone bucketsizebyte metrics were selected to be monitored simultaneously. 
Customizing the metric line graph, we selected the range of the last 3 months and assigned AWS to update the graph every 15 mins. The metric widget was then created. Another widget was added and the service metric chosen to be monitored was AWS Glue, selecting JobRun time, to know how long it takes to run a job. It was also customized for a 3 months time period. Lastly, similar steps were taken for the customization and creation of an additional widget for the web applications service using the AWS Elastic beanstalk service.
Cloudtrail is also another service that was used to monitor the users, and parties that have access to the account and use data or information in the account. A new trail called hou-curr-iss-tra-edet was created to monitor, log, and record all activities of users in the account.

# Course Completion Badge
https://www.credly.com/badges/aa311dd4-a642-43df-8d0a-3d679a9bc46c
